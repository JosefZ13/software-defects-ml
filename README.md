\# Machine Learning for Software Defect Prediction â€“ Bachelor Thesis



This repository contains the code, datasets, and analysis for my bachelor thesis:  

\*\*Machine Learning for Software Defect Prediction: A Comparative Study\*\*



---



\## ğŸ“‚ Repository Structure



software-defects-ml/

â”œâ”€â”€ data/ # NASA PROMISE datasets (JM1, KC1, KC2, PC1, CM1)

â”œâ”€â”€ notebooks/ # Jupyter notebooks (01â€“10)

â”‚ â”œâ”€â”€ 01\_data\_preprocessing.ipynb

â”‚ â”œâ”€â”€ 02\_basic\_models.ipynb

â”‚ â”œâ”€â”€ 03\_smote\_and\_boosting.ipynb

â”‚ â”œâ”€â”€ 04\_ann.ipynb

â”‚ â”œâ”€â”€ 05\_kc1\_experiments.ipynb

â”‚ â”œâ”€â”€ 06\_curves.ipynb

â”‚ â”œâ”€â”€ 07\_error\_analysis.ipynb

â”‚ â”œâ”€â”€ 08\_Hyperparameter\_Tuning.ipynb

â”‚ â”œâ”€â”€ 09\_NTuning\_vs\_Tuning.ipynb

â”‚ â””â”€â”€ 10\_final\_model\_comparison.ipynb

â”œâ”€â”€ report/ # Thesis report (PDF/Word)

â”œâ”€â”€ presentation/ # PowerPoint presentation

â”œâ”€â”€ README.md

â”œâ”€â”€ requirements.txt # Python dependencies

â””â”€â”€ .gitignore





---



\## ğŸ“Š Pipeline Overview



The project follows a step-by-step machine learning pipeline:



1\. \*\*Data Preprocessing\*\* â†’ cleaning, scaling, stratified train/test splits  

2\. \*\*Basic Models\*\* â†’ Logistic Regression, Random Forest, XGBoost, ANN  

3\. \*\*SMOTE and Boosting\*\* â†’ handling class imbalance  

4\. \*\*Artificial Neural Network\*\* â†’ ANN performance evaluation  

5\. \*\*KC1 Experiments\*\* â†’ dataset-specific experiments  

6\. \*\*Curves\*\* â†’ ROC and Precision-Recall visualization  

7\. \*\*Error Analysis\*\* â†’ FP/FN breakdowns  

8\. \*\*Hyperparameter Tuning\*\* â†’ parameter optimization  

9\. \*\*Normal vs Tuned Models\*\* â†’ comparison of performance  

10\. \*\*Final Model Comparison\*\* â†’ summary across all datasets



---



\## âš™ï¸ Setup Instructions



\### 1. Install dependencies

```bash

pip install -r requirements.txt



2\. Start Jupyter



jupyter lab



3\. Run the notebooks



Open the notebooks in /notebooks and run the cells in order.

Example (Random Forest with and without SMOTE):



&nbsp;   Shows class distribution



&nbsp;   Applies stratified split



&nbsp;   Runs baseline RF



&nbsp;   Applies SMOTE



&nbsp;   Compares results with confusion matrix, ROC and PR curves



ğŸ“ˆ Key Evaluation Metrics



&nbsp;   Accuracy â†’ overall correctness



&nbsp;   Precision â†’ how many predicted defects are truly defects



&nbsp;   Recall (Sensitivity) â†’ how many actual defects were found



&nbsp;   F1-score â†’ balance of precision and recall



&nbsp;   AUC â†’ ability to distinguish classes



ğŸ’¡ Notes for Confusion Matrix (for examiners)



&nbsp;   True Positive (TP): Predicted defect and it was a defect âœ…



&nbsp;   False Positive (FP): Predicted defect but it was clean (extra QA effort) âš ï¸



&nbsp;   False Negative (FN): Predicted clean but it was a defect (worst case â€“ missed bug) âŒ



&nbsp;   True Negative (TN): Predicted clean and it was clean âœ…



ğŸ‘‰ In defect prediction, recall (catching more defects) is often prioritized to reduce FN.

ğŸ™‹ Author



Josef Ziada

Bachelor Thesis â€“ University West, 2025

